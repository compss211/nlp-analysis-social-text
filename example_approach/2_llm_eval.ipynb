{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffcf739",
   "metadata": {},
   "source": [
    "# LLM Eval: RAG over r/changemyview Conversations (Qwen 2.5)\n",
    "\n",
    "**Goal:** load CSV from PRAW notebook, retrieve relevant branches, and prompt Qwen 2.5 to analyze *winning vs unsuccessful* rhetoric. Output structured JSON with confidence and evidence IDs.\n",
    "\n",
    "**Sections:**\n",
    "1. [Hardware & Model](#hw)  \n",
    "2. [Load Data](#data)  \n",
    "3. [Lite Retrieval](#retrieval)  \n",
    "4. [Research Questions](#rqs)  \n",
    "5. [Prompt & Generate](#prompt)  \n",
    "6. [Parse JSON & Save](#save)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d31c4",
   "metadata": {},
   "source": [
    "<a id='hw'></a>\n",
    "\n",
    "# Hardware & Model\n",
    "\n",
    "Pick Qwen 2.5 size by available GPU memory (7B / 14B / 32B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed:\n",
    "# !pip install -q torch transformers pandas scikit-learn tiktoken\n",
    "\n",
    "import torch, os, time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def pick_qwen_by_memory(vram_gb: float) -> str:\n",
    "    if vram_gb >= 40:  # roomy\n",
    "        return \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "    if vram_gb >= 18:\n",
    "        return \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "    return \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "def detect_device():\n",
    "    if torch.cuda.is_available():\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        return \"cuda\", total\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\", 8\n",
    "    return \"cpu\", 0\n",
    "\n",
    "DEVICE, VRAM_GB = detect_device()\n",
    "MODEL_NAME = pick_qwen_by_memory(VRAM_GB)\n",
    "DTYPE = torch.bfloat16 if DEVICE in {\"cuda\",\"mps\"} else torch.float32\n",
    "\n",
    "print(f\"Device: {DEVICE}, VRAM≈{VRAM_GB:.1f} GB\")\n",
    "print(f\"Model:  {MODEL_NAME}\")\n",
    "\n",
    "# Lazy load on first use to keep the notebook snappy\n",
    "_tokenizer = _model = None\n",
    "def load_model():\n",
    "    global _tokenizer, _model\n",
    "    if _model is None:\n",
    "        _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        _model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=DTYPE,\n",
    "            device_map=\"auto\" if DEVICE!=\"cpu\" else None,\n",
    "        )\n",
    "    return _tokenizer, _model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a6c5b8",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "\n",
    "# Load Data\n",
    "\n",
    "Uses the **same CSV schema** saved by the PRAW notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "CSV_PATH = \"cmv_threads.csv\"  # must match the PRAW notebook output\n",
    "assert os.path.exists(CSV_PATH), f\"CSV not found: {CSV_PATH} (run the PRAW notebook first)\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Minimal cleaning\n",
    "df['text'] = df['body'].fillna(\"\").astype(str)\n",
    "print(df.head(2)[['post_id','comment_id','parent_id','branch_type','text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea010c",
   "metadata": {},
   "source": [
    "<a id='retrieval'></a>\n",
    "\n",
    "# Lite Retrieval\n",
    "\n",
    "Tiny TF‑IDF retrieval to keep things dependency‑light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Make simple 'doc' units: each comment row is a doc.\n",
    "docs = df[['post_id','comment_id','branch_type','text']].copy()\n",
    "vec = TfidfVectorizer(min_df=2, max_df=0.95)\n",
    "X = vec.fit_transform(docs['text'])\n",
    "\n",
    "def retrieve(q: str, k: int = 6):\n",
    "    qv = vec.transform([q])\n",
    "    sims = cosine_similarity(qv, X).ravel()\n",
    "    top = np.argsort(sims)[-k:][::-1]\n",
    "    return docs.iloc[top].assign(score=sims[top])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e0225",
   "metadata": {},
   "source": [
    "<a id='rqs'></a>\n",
    "\n",
    "# Research Questions\n",
    "\n",
    "Start with a few templates or generate via the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "RQS = [\n",
    "    \"Which rhetorical strategies most often precede a granted delta?\",\n",
    "    \"Do empathetic openers correlate with higher delta rates?\",\n",
    "    \"Are hedging phrases more common in winning than unsuccessful branches?\",\n",
    "    \"Does asking clarifying questions increase persuasion?\",\n",
    "    \"How does citing personal experience affect outcomes?\",\n",
    "    \"What tone differences (polite vs confrontational) predict success?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78951be7",
   "metadata": {},
   "source": [
    "<a id='prompt'></a>\n",
    "\n",
    "# Prompt & Generate\n",
    "\n",
    "Short helper to call Qwen and request **valid JSON**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "\n",
    "def format_context(rows):\n",
    "    # Keep it tiny: id + short text\n",
    "    out = []\n",
    "    for _, r in rows.iterrows():\n",
    "        snippet = r['text']\n",
    "        if len(snippet) > 420:  # trim\n",
    "            snippet = snippet[:420] + \"…\"\n",
    "        out.append({\n",
    "            \"post_id\": r['post_id'],\n",
    "            \"comment_id\": r['comment_id'],\n",
    "            \"branch_type\": r['branch_type'],\n",
    "            \"text\": snippet\n",
    "        })\n",
    "    return out\n",
    "\n",
    "SYSTEM = \"You are a careful research assistant. Return ONLY JSON.\"\n",
    "\n",
    "PROMPT_TMPL = (\n",
    "    \"You analyze persuasion in r/changemyview conversation snippets.\\n\"\n",
    "    \"Task: For the research question: {question}\\n\"\n",
    "    \"Use the provided snippets (some winning, some unsuccessful).\\n\"\n",
    "    \"Identify patterns, techniques, and cite evidence by comment_id.\\n\"\n",
    "    \"Return STRICT JSON with keys: \"\n",
    "    '{\"question\": \"...\", \"claim\": \"...\", \"techniques\": [\"...\"], '\n",
    "    '\"evidence_ids\": [\"comment_id\", ...], \"confidence\": 0.0, \"notes\": \"...\"}\\n\\n'\n",
    "    \"Snippets JSON:\\n{snippets}\\n\"\n",
    ")\n",
    "\n",
    "def generate_json_response(question: str, k: int = 6, max_new_tokens: int = 512):\n",
    "    # get top-K docs per question, mix winning/unsuccessful if possible\n",
    "    rows = retrieve(question, k=k)\n",
    "    ctx = format_context(rows)\n",
    "    prompt = PROMPT_TMPL.format(question=question, snippets=json.dumps(ctx, ensure_ascii=False))\n",
    "\n",
    "    tok, mdl = load_model()\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "    with torch.no_grad():\n",
    "        out = mdl.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # Try to extract JSON (robust to accidental pre/post text)\n",
    "    m = re.search(r\"\\{[\\s\\S]*\\}\", text)\n",
    "    if not m:\n",
    "        raise ValueError(\"No JSON found in model output\")\n",
    "    payload = json.loads(m.group(0))\n",
    "    payload[\"retrieved\"] = ctx  # add context for traceability\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d5975",
   "metadata": {},
   "source": [
    "<a id='save'></a>\n",
    "\n",
    "# Parse & Save\n",
    "\n",
    "Run a few questions, write JSONL. Keep artifacts small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a76942",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for q in RQS[:3]:  # demo; adjust as needed\n",
    "    try:\n",
    "        pj = generate_json_response(q, k=8)\n",
    "        results.append(pj)\n",
    "        print(f\"✓ {q}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ {q}: {e}\")\n",
    "\n",
    "out_path = \"qwen_eval.jsonl\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in results:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(results)} records → {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
