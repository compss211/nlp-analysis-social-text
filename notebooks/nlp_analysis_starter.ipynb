{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5: Multi-Document NLP Analysis - r/ChangeMyView Conversations\n",
    "\n",
    "**Name:** [Your Name Here]\n",
    "\n",
    "**Date:** [Date]\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "Building on the NLP fundamentals and word embeddings labs, you will analyze conversations in r/ChangeMyView by examining both posts and comments together. This multi-document analysis will help you understand how online discourse unfolds across linked texts.\n",
    "\n",
    "### What You'll Do:\n",
    "1. **Link datasets** - Connect posts with their comments\n",
    "2. **Multi-document preprocessing** - Clean both posts and comments\n",
    "3. **Conversation analysis** - Compare language patterns between posts and responses\n",
    "4. **Social dynamics** - Use word embeddings to analyze how perspectives shift in conversations\n",
    "5. **Discourse patterns** - Identify how different topics generate different types of discussions\n",
    "\n",
    "### Skills Applied:\n",
    "- Multi-document NLP techniques\n",
    "- Dataset linking and relational analysis\n",
    "- Word embeddings for social analysis (from lab)\n",
    "- Conversation and discourse analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries\n",
    "\n",
    "*Building on the tools from your NLP labs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries (from labs)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Word embeddings (from word embeddings lab)\n",
    "import gensim.downloader as api\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Download required data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Link the Datasets\n",
    "\n",
    "*Multi-document analysis requires linking related texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "posts_df = pd.read_csv('../data/changemyview_posts.csv')\n",
    "comments_df = pd.read_csv('../data/cmv_comments.csv')\n",
    "\n",
    "print(f\"Posts dataset: {posts_df.shape}\")\n",
    "print(f\"Comments dataset: {comments_df.shape}\")\n",
    "\n",
    "# TODO: Explore the structure of both datasets\n",
    "# What columns do they share for linking?\n",
    "print(f\"\\nPosts columns: {posts_df.columns.tolist()}\")\n",
    "print(f\"Comments columns: {comments_df.columns.tolist()}\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nSample post:\")\n",
    "print(posts_df[['title', 'score', 'num_comments']].head(1))\n",
    "print(f\"\\nSample comment:\")\n",
    "print(comments_df[['body', 'score']].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Link the datasets\n",
    "# How do posts connect to their comments?\n",
    "# Hint: Look for shared ID columns\n",
    "\n",
    "# Find the linking column\n",
    "linking_columns = set(posts_df.columns) & set(comments_df.columns)\n",
    "print(f\"\\nShared columns for linking: {linking_columns}\")\n",
    "\n",
    "# TODO: Analyze the post-comment relationships\n",
    "# How many posts have comments in our dataset?\n",
    "\n",
    "print(f\"\\nUnique posts in posts_df: {posts_df['id'].nunique()}\")\n",
    "print(f\"Unique link_ids in comments_df: {comments_df['link_id'].nunique()}\")\n",
    "\n",
    "# Check overlap\n",
    "posts_with_comments = posts_df['id'].isin(comments_df['link_id'])\n",
    "print(f\"Posts with comments in our dataset: {posts_with_comments.sum()} ({posts_with_comments.mean()*100:.1f}%)\")\n",
    "\n",
    "# Sample conversation preview\n",
    "sample_post_id = posts_df['id'].iloc[0]\n",
    "print(f\"\\nSample conversation:\")\n",
    "print(f\"Post: {posts_df[posts_df['id'] == sample_post_id]['title'].iloc[0][:100]}...\")\n",
    "related_comments = comments_df[comments_df['link_id'] == sample_post_id]\n",
    "print(f\"Number of comments: {len(related_comments)}\")\n",
    "if len(related_comments) > 0:\n",
    "    print(f\"Sample comment: {related_comments['body'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Document Text Preprocessing\n",
    "\n",
    "*Apply consistent preprocessing to both posts and comments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function (from labs)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess Reddit text\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove Reddit-specific content\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'/u/\\w+|/r/\\w+', '', text)\n",
    "    text = re.sub(r'\\[removed\\]|\\[deleted\\]', '', text)\n",
    "    \n",
    "    # Clean text\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# TODO: Preprocess both posts and comments\n",
    "# Apply consistent cleaning to both datasets\n",
    "\n",
    "# Posts preprocessing\n",
    "posts_df['combined_text'] = posts_df['title'].fillna('') + ' ' + posts_df['selftext'].fillna('')\n",
    "posts_df['clean_text'] = posts_df['combined_text'].apply(clean_text)\n",
    "\n",
    "# Comments preprocessing  \n",
    "comments_df['clean_text'] = comments_df['body'].apply(clean_text)\n",
    "\n",
    "# Remove empty texts\n",
    "posts_clean = posts_df[posts_df['clean_text'].str.len() > 10].copy()\n",
    "comments_clean = comments_df[comments_df['clean_text'].str.len() > 10].copy()\n",
    "\n",
    "print(f\"Clean posts: {len(posts_clean)} (removed {len(posts_df) - len(posts_clean)})\")\n",
    "print(f\"Clean comments: {len(comments_clean)} (removed {len(comments_df) - len(comments_clean)})\")\n",
    "\n",
    "# Show preprocessing examples\n",
    "print(f\"\\nSample clean post: {posts_clean['clean_text'].iloc[0][:100]}...\")\n",
    "print(f\"Sample clean comment: {comments_clean['clean_text'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization (using approach from labs)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['cmv', 'change', 'view', 'mind', 'think', 'would', 'could', 'should'])\n",
    "\n",
    "def tokenize_text(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [t for t in tokens if t.isalpha() and len(t) > 2 and t not in stop_words]\n",
    "\n",
    "# Apply tokenization to both datasets\n",
    "posts_clean['tokens'] = posts_clean['clean_text'].apply(tokenize_text)\n",
    "comments_clean['tokens'] = comments_clean['clean_text'].apply(tokenize_text)\n",
    "\n",
    "print(f\"Sample tokens from post: {posts_clean['tokens'].iloc[0][:10]}\")\n",
    "print(f\"Sample tokens from comment: {comments_clean['tokens'].iloc[0][:10]}\")\n",
    "\n",
    "# Filter to posts/comments with sufficient tokens\n",
    "posts_analysis = posts_clean[posts_clean['tokens'].str.len() > 5].copy()\n",
    "comments_analysis = comments_clean[comments_clean['tokens'].str.len() > 3].copy()\n",
    "\n",
    "print(f\"\\nDatasets for analysis:\")\n",
    "print(f\"Posts: {len(posts_analysis)}\")\n",
    "print(f\"Comments: {len(comments_analysis)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparative Word Analysis\n",
    "\n",
    "*Compare language patterns between posts and comments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare word frequencies between posts and comments\n",
    "# What words are more common in posts vs comments?\n",
    "\n",
    "# Get word frequencies for each document type\n",
    "post_tokens = [token for tokens in posts_analysis['tokens'] for token in tokens]\n",
    "comment_tokens = [token for tokens in comments_analysis['tokens'] for token in tokens]\n",
    "\n",
    "post_freq = Counter(post_tokens)\n",
    "comment_freq = Counter(comment_tokens)\n",
    "\n",
    "print(f\"Total post tokens: {len(post_tokens):,}\")\n",
    "print(f\"Total comment tokens: {len(comment_tokens):,}\")\n",
    "\n",
    "# Find words that are more common in posts vs comments\n",
    "post_top = set([word for word, _ in post_freq.most_common(100)])\n",
    "comment_top = set([word for word, _ in comment_freq.most_common(100)])\n",
    "\n",
    "post_distinctive = post_top - comment_top\n",
    "comment_distinctive = comment_top - post_top\n",
    "\n",
    "print(f\"\\nWords more distinctive to posts: {list(post_distinctive)[:10]}\")\n",
    "print(f\"Words more distinctive to comments: {list(comment_distinctive)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the comparison\n",
    "# Create a comparative visualization of post vs comment language\n",
    "\n",
    "# Select common words for comparison\n",
    "common_words = list((post_top & comment_top) - {'people', 'think', 'like', 'really', 'know', 'right', 'way'})[:10]\n",
    "\n",
    "# Get normalized frequencies\n",
    "post_total = len(post_tokens)\n",
    "comment_total = len(comment_tokens)\n",
    "\n",
    "post_rates = [post_freq[word] / post_total * 1000 for word in common_words]\n",
    "comment_rates = [comment_freq[word] / comment_total * 1000 for word in common_words]\n",
    "\n",
    "# Create comparison plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(common_words))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, post_rates, width, label='Posts', alpha=0.8)\n",
    "ax.bar(x + width/2, comment_rates, width, label='Comments', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Words')\n",
    "ax.set_ylabel('Rate per 1000 tokens')\n",
    "ax.set_title('Word Usage: Posts vs Comments')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(common_words, rotation=45)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: What does this comparison tell us about how people communicate differently \n",
    "# in posts (starting discussions) vs comments (responding to discussions)?\n",
    "\n",
    "# Your analysis here:\n",
    "print(\"\\nAnalysis Questions:\")\n",
    "print(\"1. What words are more common in posts vs comments?\")\n",
    "print(\"2. What does this suggest about different communication purposes?\")\n",
    "print(\"3. How might this relate to persuasion vs response dynamics?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversation Analysis\n",
    "\n",
    "*Analyze how conversations unfold between posts and their comments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create linked post-comment pairs for conversation analysis\n",
    "# Focus on posts that have multiple comments\n",
    "\n",
    "# Find posts with substantial comment discussions\n",
    "post_comment_counts = comments_analysis.groupby('link_id').size()\n",
    "posts_with_discussion = post_comment_counts[post_comment_counts >= 5].index\n",
    "\n",
    "conversation_posts = posts_analysis[posts_analysis['id'].isin(posts_with_discussion)].copy()\n",
    "conversation_comments = comments_analysis[comments_analysis['link_id'].isin(posts_with_discussion)].copy()\n",
    "\n",
    "print(f\"Posts with 5+ comments: {len(conversation_posts)}\")\n",
    "print(f\"Comments in these conversations: {len(conversation_comments)}\")\n",
    "\n",
    "# Create conversation pairs\n",
    "conversations = []\n",
    "for post_id in conversation_posts['id'].head(10):  # Sample 10 conversations\n",
    "    post_text = conversation_posts[conversation_posts['id'] == post_id]['clean_text'].iloc[0]\n",
    "    related_comments = conversation_comments[conversation_comments['link_id'] == post_id]['clean_text'].tolist()\n",
    "    \n",
    "    conversations.append({\n",
    "        'post_id': post_id,\n",
    "        'post_text': post_text,\n",
    "        'comments': related_comments,\n",
    "        'num_comments': len(related_comments)\n",
    "    })\n",
    "\n",
    "print(f\"\\nSample conversation structure:\")\n",
    "print(f\"Post: {conversations[0]['post_text'][:100]}...\")\n",
    "print(f\"First comment: {conversations[0]['comments'][0][:100]}...\")\n",
    "print(f\"Comments in conversation: {conversations[0]['num_comments']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze conversation dynamics\n",
    "# How do comment responses relate to the original post?\n",
    "\n",
    "def calculate_text_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two texts using TF-IDF\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        return similarity\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Calculate post-comment similarities\n",
    "similarities = []\n",
    "for conv in conversations[:5]:  # Sample 5 conversations\n",
    "    post_text = conv['post_text']\n",
    "    for comment_text in conv['comments'][:3]:  # Top 3 comments per post\n",
    "        sim = calculate_text_similarity(post_text, comment_text)\n",
    "        similarities.append(sim)\n",
    "\n",
    "print(f\"\\nPost-Comment Similarities:\")\n",
    "print(f\"Mean similarity: {np.mean(similarities):.3f}\")\n",
    "print(f\"Range: {np.min(similarities):.3f} - {np.max(similarities):.3f}\")\n",
    "print(f\"\\nInterpretation: Higher similarity = comments that closely mirror post language\")\n",
    "print(f\"Lower similarity = comments that introduce new concepts/perspectives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word Embeddings Analysis\n",
    "\n",
    "*Apply word embeddings from the lab to analyze discourse patterns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load word embeddings (from your word embeddings lab)\n",
    "# Use embeddings to analyze semantic patterns in posts vs comments\n",
    "\n",
    "print(\"Loading word embeddings...\")\n",
    "model = api.load('glove-wiki-gigaword-50')  # Same as lab\n",
    "print(f\"Model loaded with {len(model.key_to_index):,} words\")\n",
    "\n",
    "# TODO: Define semantic axes for social discourse analysis\n",
    "# Use the approach from your word embeddings lab\n",
    "\n",
    "def create_semantic_axis(positive_words, negative_words):\n",
    "    pos_vectors = [model[word] for word in positive_words if word in model]\n",
    "    neg_vectors = [model[word] for word in negative_words if word in model]\n",
    "    \n",
    "    if not pos_vectors or not neg_vectors:\n",
    "        return None\n",
    "    \n",
    "    pos_mean = np.mean(pos_vectors, axis=0)\n",
    "    neg_mean = np.mean(neg_vectors, axis=0)\n",
    "    axis = pos_mean - neg_mean\n",
    "    return axis / np.linalg.norm(axis)\n",
    "\n",
    "# Create discourse axes\n",
    "axes = {\n",
    "    'agreement_disagreement': create_semantic_axis(\n",
    "        ['agree', 'support', 'correct', 'right'],\n",
    "        ['disagree', 'oppose', 'wrong', 'incorrect']\n",
    "    ),\n",
    "    'emotional_rational': create_semantic_axis(\n",
    "        ['feel', 'emotion', 'heart', 'passionate'],\n",
    "        ['logic', 'rational', 'evidence', 'analysis']\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Created {len([a for a in axes.values() if a is not None])} semantic axes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Project key debate words onto semantic axes\n",
    "# Analyze how posts vs comments differ in their semantic positioning\n",
    "\n",
    "def project_word_onto_axis(word, axis):\n",
    "    if word not in model or axis is None:\n",
    "        return None\n",
    "    word_vector = model[word] / np.linalg.norm(model[word])\n",
    "    return np.dot(word_vector, axis)\n",
    "\n",
    "# Get distinctive words from posts and comments\n",
    "post_distinctive_words = [word for word, count in Counter(post_tokens).most_common(30) \n",
    "                        if word in model and count > 20]\n",
    "comment_distinctive_words = [word for word, count in Counter(comment_tokens).most_common(30) \n",
    "                           if word in model and count > 50]\n",
    "\n",
    "# Project onto agreement-disagreement axis\n",
    "if axes['agreement_disagreement'] is not None:\n",
    "    print(\"\\nPost words on Agreement-Disagreement axis:\")\n",
    "    for word in post_distinctive_words[:10]:\n",
    "        proj = project_word_onto_axis(word, axes['agreement_disagreement'])\n",
    "        if proj is not None:\n",
    "            direction = \"→ agreement\" if proj > 0 else \"→ disagreement\"\n",
    "            print(f\"  {word}: {proj:.3f} {direction}\")\n",
    "    \n",
    "    print(\"\\nComment words on Agreement-Disagreement axis:\")\n",
    "    for word in comment_distinctive_words[:10]:\n",
    "        proj = project_word_onto_axis(word, axes['agreement_disagreement'])\n",
    "        if proj is not None:\n",
    "            direction = \"→ agreement\" if proj > 0 else \"→ disagreement\"\n",
    "            print(f\"  {word}: {proj:.3f} {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Social Dynamics Analysis\n",
    "\n",
    "*Analyze patterns that reveal social and persuasive dynamics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze social dynamics patterns\n",
    "# Question: How do different types of posts generate different conversation patterns?\n",
    "\n",
    "# Categorize posts by engagement type\n",
    "posts_analysis['engagement_type'] = 'low'\n",
    "posts_analysis.loc[posts_analysis['num_comments'] > posts_analysis['num_comments'].quantile(0.8), 'engagement_type'] = 'high_discussion'\n",
    "posts_analysis.loc[(posts_analysis['score'] > posts_analysis['score'].quantile(0.8)) & \n",
    "                   (posts_analysis['num_comments'] < posts_analysis['num_comments'].quantile(0.5)), 'engagement_type'] = 'high_agreement'\n",
    "\n",
    "print(\"Engagement type distribution:\")\n",
    "print(posts_analysis['engagement_type'].value_counts())\n",
    "\n",
    "# TODO: Analyze language patterns by engagement type\n",
    "for eng_type in ['high_discussion', 'high_agreement']:\n",
    "    posts_subset = posts_analysis[posts_analysis['engagement_type'] == eng_type]\n",
    "    if len(posts_subset) > 5:\n",
    "        subset_tokens = [token for tokens in posts_subset['tokens'] for token in tokens]\n",
    "        top_words = Counter(subset_tokens).most_common(10)\n",
    "        print(f\"\\nTop words in {eng_type} posts:\")\n",
    "        print([word for word, count in top_words])\n",
    "\n",
    "# Social dynamics questions for analysis:\n",
    "print(\"\\nSocial Dynamics Questions:\")\n",
    "print(\"1. What language patterns distinguish posts that generate debate vs agreement?\")\n",
    "print(\"2. How do comment responses differ from original post language?\")\n",
    "print(\"3. What semantic patterns suggest successful persuasion or perspective change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a visualization of conversation dynamics\n",
    "# Compare semantic positioning of posts vs their comments\n",
    "\n",
    "# Sample a few conversations for detailed analysis\n",
    "sample_conversations = conversations[:3]\n",
    "\n",
    "for i, conv in enumerate(sample_conversations):\n",
    "    print(f\"\\nConversation {i+1} Analysis:\")\n",
    "    print(f\"Post preview: {conv['post_text'][:80]}...\")\n",
    "    \n",
    "    # Get key words from post and comments\n",
    "    post_words = tokenize_text(conv['post_text'])[:10]\n",
    "    comment_words = []\n",
    "    for comment in conv['comments'][:3]:\n",
    "        comment_words.extend(tokenize_text(comment)[:5])\n",
    "    \n",
    "    # Find words that appear in both\n",
    "    shared_concepts = set(post_words) & set(comment_words) & set(model.key_to_index.keys())\n",
    "    \n",
    "    if shared_concepts:\n",
    "        print(f\"Shared concepts: {list(shared_concepts)[:5]}\")\n",
    "        \n",
    "        # Project shared concepts onto semantic axes\n",
    "        if axes['agreement_disagreement'] is not None:\n",
    "            print(\"Semantic analysis of shared concepts:\")\n",
    "            for word in list(shared_concepts)[:3]:\n",
    "                proj = project_word_onto_axis(word, axes['agreement_disagreement'])\n",
    "                if proj is not None:\n",
    "                    tendency = \"agreement\" if proj > 0 else \"disagreement\"\n",
    "                    print(f\"  {word}: tends toward {tendency} ({proj:.2f})\")\n",
    "    else:\n",
    "        print(\"No shared concepts found in embeddings vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion Quality Analysis\n",
    "\n",
    "*Analyze what makes productive vs unproductive online discussions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze discussion quality indicators\n",
    "# What linguistic patterns are associated with productive discussions?\n",
    "\n",
    "# Define quality indicators based on CMV context\n",
    "quality_indicators = {\n",
    "    'reasoning_words': ['because', 'therefore', 'evidence', 'research', 'study', 'data', 'analysis'],\n",
    "    'perspective_words': ['understand', 'perspective', 'viewpoint', 'consider', 'acknowledge'],\n",
    "    'civility_words': ['respect', 'appreciate', 'thank', 'interesting', 'valid'],\n",
    "    'change_words': ['delta', 'changed', 'convinced', 'reconsidered', 'modified']\n",
    "}\n",
    "\n",
    "def count_quality_indicators(text, indicator_list):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    text_lower = str(text).lower()\n",
    "    return sum(1 for word in indicator_list if word in text_lower)\n",
    "\n",
    "# Apply quality analysis to posts and comments\n",
    "for category, words in quality_indicators.items():\n",
    "    posts_analysis[f'{category}_count'] = posts_analysis['clean_text'].apply(\n",
    "        lambda x: count_quality_indicators(x, words)\n",
    "    )\n",
    "    comments_analysis[f'{category}_count'] = comments_analysis['clean_text'].apply(\n",
    "        lambda x: count_quality_indicators(x, words)\n",
    "    )\n",
    "\n",
    "# Compare quality indicators between posts and comments\n",
    "print(\"Quality Indicators Comparison:\")\n",
    "for category in quality_indicators.keys():\n",
    "    post_avg = posts_analysis[f'{category}_count'].mean()\n",
    "    comment_avg = comments_analysis[f'{category}_count'].mean()\n",
    "    print(f\"{category}: Posts {post_avg:.2f}, Comments {comment_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze which posts generate the highest quality discussions\n",
    "# Look at posts with high comment engagement and quality indicators\n",
    "\n",
    "posts_analysis['total_quality'] = (posts_analysis['reasoning_words_count'] + \n",
    "                                  posts_analysis['perspective_words_count'] + \n",
    "                                  posts_analysis['civility_words_count'])\n",
    "\n",
    "# High quality discussion posts\n",
    "high_quality_posts = posts_analysis[\n",
    "    (posts_analysis['num_comments'] > posts_analysis['num_comments'].quantile(0.8)) &\n",
    "    (posts_analysis['total_quality'] > 0)\n",
    "]\n",
    "\n",
    "print(f\"\\nHigh-quality discussion posts: {len(high_quality_posts)}\")\n",
    "if len(high_quality_posts) > 0:\n",
    "    print(\"Sample high-quality post titles:\")\n",
    "    for title in high_quality_posts['title'].head(3):\n",
    "        print(f\"- {title[:100]}...\")\n",
    "\n",
    "# Compare quality indicators\n",
    "print(f\"\\nQuality comparison:\")\n",
    "print(f\"Average reasoning words - All posts: {posts_analysis['reasoning_words_count'].mean():.2f}\")\n",
    "print(f\"Average reasoning words - High quality: {high_quality_posts['reasoning_words_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Your Social Dynamics Interpretation\n",
    "\n",
    "*Connect your findings to broader questions about online discourse*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Synthesize your findings to address social science questions\n",
    "# Use your multi-document NLP analysis to answer questions about online discourse\n",
    "\n",
    "print(\"Social Dynamics Questions to Address:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. How does language differ between posts (opinion statements) and comments (responses)?\")\n",
    "print(\"2. What linguistic patterns are associated with productive vs unproductive discussions?\")\n",
    "print(\"3. How do semantic patterns reveal different types of engagement (agreement, debate, etc.)?\")\n",
    "print(\"4. What does this analysis reveal about how people engage in persuasion online?\")\n",
    "print(\"5. How might these patterns inform design of better online discussion platforms?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"YOUR ANALYSIS AND INTERPRETATION:\")\n",
    "print(\"\\nBased on your multi-document NLP analysis above, write 2-3 paragraphs addressing these questions.\")\n",
    "print(\"Connect your specific findings (word patterns, semantic analysis, conversation dynamics)\")\n",
    "print(\"to broader insights about online discourse and social persuasion.\")\n",
    "print(\"\\nConsider:\")\n",
    "print(\"- What did the post vs comment language comparison reveal?\")\n",
    "print(\"- How did the word embeddings analysis show different types of discourse?\")\n",
    "print(\"- What patterns emerged in quality discussions vs lower-quality ones?\")\n",
    "print(\"- How do these findings relate to theories about online deliberation and opinion change?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Your Multi-Document Analysis and Social Science Interpretation\n",
    "\n",
    "### TODO: Complete this section with your analysis\n",
    "\n",
    "Based on your multi-document NLP analysis above, write a 3-4 paragraph interpretation that addresses:\n",
    "\n",
    "1. **Language Differences**: What did you discover about how language differs between posts (initial arguments) and comments (responses)? What does this suggest about different communicative purposes?\n",
    "\n",
    "2. **Conversation Dynamics**: How do conversations unfold between posts and comments? What patterns did you find in terms of semantic similarity, quality indicators, and engagement types?\n",
    "\n",
    "3. **Social Dynamics**: Using your word embeddings analysis, what did you learn about how different types of discourse (agreement vs disagreement, emotional vs rational) manifest in CMV discussions?\n",
    "\n",
    "4. **Broader Implications**: What do these patterns tell us about online deliberation and opinion change? How might these insights inform the design of better platforms for constructive discourse?\n",
    "\n",
    "**Your interpretation here:**\n",
    "\n",
    "[Write your 3-4 paragraph interpretation here, drawing specifically on your analysis results]\n",
    "\n",
    "### Key Evidence from Your Analysis\n",
    "\n",
    "TODO: Reference specific findings from your analysis:\n",
    "- Quote distinctive words/patterns you found\n",
    "- Cite specific semantic axis results\n",
    "- Reference conversation similarity scores\n",
    "- Mention quality indicator patterns\n",
    "\n",
    "### Additional Multi-Document Analysis (Optional)\n",
    "\n",
    "Consider extending your analysis with:\n",
    "- Temporal patterns in conversations\n",
    "- Comment thread depth analysis\n",
    "- Cross-referencing high-quality posts with comment patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optional Extensions\n",
    "# 1. Analyze comment thread structures (parent-child relationships)\n",
    "# 2. Look for evidence of opinion change in comment sequences\n",
    "# 3. Compare early vs late comments in discussions\n",
    "# 4. Analyze posts that received \"delta\" awards (successful view changes)\n",
    "\n",
    "# Your additional analysis here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions and Implications\n",
    "\n",
    "### Key Takeaways from Multi-Document Analysis\n",
    "\n",
    "TODO: List 4-5 key takeaways from your multi-document analysis:\n",
    "1. **Language Patterns**: [Your finding about post vs comment language]\n",
    "2. **Conversation Dynamics**: [Your finding about how discussions unfold]\n",
    "3. **Semantic Patterns**: [Your finding from word embeddings analysis]\n",
    "4. **Quality Indicators**: [Your finding about productive discussions]\n",
    "5. **Social Implications**: [Your broader insight about online discourse]\n",
    "\n",
    "### Methodological Insights\n",
    "\n",
    "TODO: Reflect on the multi-document NLP approach:\n",
    "- What did linking datasets reveal that single-document analysis would miss?\n",
    "- How did word embeddings enhance your understanding of social dynamics?\n",
    "- What challenges did you encounter in conversation analysis?\n",
    "\n",
    "### Implications for Online Platform Design\n",
    "\n",
    "TODO: Based on your findings, what recommendations would you make for designing better discussion platforms?\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "TODO: Discuss limitations and extensions:\n",
    "- Dataset limitations (time period, platform-specific effects)\n",
    "- Methodological limitations (preprocessing choices, semantic axes)\n",
    "- Future work (longitudinal analysis, cross-platform comparison, intervention studies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
